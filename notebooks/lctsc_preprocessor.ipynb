{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AkmY9FMFbN1J"
   },
   "source": [
    "## Data Acquisition (Assumes Data is Already Downloaded)\n",
    "\n",
    "This notebook assumes you have already downloaded the LCTSC dataset from the TCIA. Instructions for downloading can be found on the TCIA website: [https://www.cancerimagingarchive.net/collections/lung-ct-segmentation-challenge-lctsc/](https://www.cancerimagingarchive.net/collections/lung-ct-segmentation-challenge-lctsc/). Due to the size of medical imaging data and the complexities of direct programmatic download within a notebook environment, we will focus on analyzing data that is assumed to be locally stored.\n",
    "\n",
    "### Directory Structure:\n",
    "It's recommended to organize your downloaded data into patient-specific directories. For example:\n",
    "```\n",
    " LCTSC/\n",
    "     LCTSC-Test-S0001/\n",
    "         1-001.dcm\n",
    "         1-002.dcm\n",
    "         ...\n",
    "     LCTSC-Test-S0002/\n",
    "         ...\n",
    "     ...\n",
    "```\n",
    "\n",
    "## Define the root directory where your LCTSC data is stored:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ea3tM872Znd3"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets install the python module of this course\n",
    "\n",
    "Download from github the library to manage the LCTSC dataset. Have a look at the repository [here](https://github.com/AI-For-Experimental-And-Applied-Physics/LCTSCProject2024)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/AI-For-Experimental-And-Applied-Physics/LCTSCProject2024.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P1aIQcbHawxI"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook will use the downloaded library to generate the numpy arrays to be used for the training of the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SwwqnwygbBD2"
   },
   "outputs": [],
   "source": [
    "!pip install -r LCTSCProject2024/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Required Modules and Append Git Repository to Path\n",
    "\n",
    "In the next section, the necessary Python modules are imported, and the path to the cloned Git repository is appended to the system path. This ensures that the custom library for managing the LCTSC dataset can be accessed and utilized in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add the LCTSCProject2024 parent directory to the Python path\n",
    "sys.path.append(\"LCTSCProject2024/\")\n",
    "\n",
    "import pandas as pd\n",
    "from lctsc_preprocessor.utils import preprocess_case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata and Raw Data Paths Initialization\n",
    "\n",
    "In the next block, the relevant variables are instantiated. These include the path to the metadata CSV file (`metadata_file`) and the directory containing the raw data (`raw_data_path`). Additionally, a flag (`plot`) is set to enable or disable plotting during preprocessing. These variables are essential for organizing and processing the LCTSC dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_file = \"PATH/TO/YOUR/lctsc_metadata.csv\"  # Replace with the actual path to your metadata file\n",
    "metadata_df = pd.read_csv(metadata_file)\n",
    "\n",
    "raw_data_path = \"PATH/TO/YOUR/RAW/DATA\"  # Replace with the actual path to your raw data directory\n",
    "plot = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Metadata and Populating Patient Dictionary\n",
    "\n",
    "In the next code block, the metadata CSV file is read into a DataFrame, and a dictionary (`patient_dict`) is populated. Each patient is associated with their unique identifier (`patient_id`) and has corresponding CT and RTSTRUCT file paths. This structure facilitates efficient data organization and access for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill the patient dict variable\n",
    "patient_dict = {}\n",
    "\n",
    "# Method 2: Using itertuples() (More efficient than iterrows())\n",
    "for row in metadata_df.itertuples():\n",
    "    patient_id = getattr(row, '_5') # unique Name\n",
    "    file_type = getattr(row, 'Modality') # check if RTSTRUCT or CT\n",
    "    file_location = getattr(row, '_16') # Path to the images\n",
    "    if patient_id in patient_dict.keys() :\n",
    "        patient_dict[patient_id][file_type] = file_location\n",
    "    else:\n",
    "        patient_dict[patient_id] = {}\n",
    "        patient_dict[patient_id][file_type] = file_location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function `preprocess_case`\n",
    "\n",
    "The `preprocess_case` function is a utility from the `lctsc_preprocessor.utils` module. It is used to process individual patient cases by taking the CT and RTSTRUCT file paths as inputs. This function performs the following tasks:\n",
    "\n",
    "1. **Data Preprocessing**: It processes the CT images and associated RTSTRUCT files to extract relevant information.\n",
    "2. **Numpy Array Generation**: Converts the processed data into numpy arrays, which are essential for training machine learning models, particularly neural networks.\n",
    "3. **Visualization (Optional)**: If the `plot` flag is set to `True`, it generates visualizations of the processed data for verification and debugging purposes.\n",
    "\n",
    "In the next block, we iterate through the `patient_dict` dictionary, which contains patient-specific data, and use the `preprocess_case` function to generate the numpy arrays required for training the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in patient_dict.items():\n",
    "    print(Path(raw_data_path).joinpath(v[\"CT\"]))\n",
    "    preprocess_case(\n",
    "        case_id = k,\n",
    "        ct_path = Path(raw_data_path).joinpath(v[\"CT\"]),\n",
    "        rtstruct_path = Path(raw_data_path).joinpath(v[\"RTSTRUCT\"]).joinpath('1-1.dcm'),\n",
    "        plot=plot\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student Exercises\n",
    "\n",
    "In this section, you will perform tasks to deepen your understanding of the data preprocessing pipeline and prepare the data for training a neural network. Follow the steps below and answer the questions:\n",
    "\n",
    "1. **Create a New Notebook**:\n",
    "    - Write a new Jupyter Notebook that reads the numpy files generated in the previous steps.\n",
    "    - Explore the contents of the numpy files and document your findings.\n",
    "\n",
    "2. **Analyze the Data**:\n",
    "    - Determine the shape of the pixels in mmÂ³. Is there metadata or information in the numpy files that provides this detail?\n",
    "    - Check if all the images have the same shape. If not, document the differences and consider how this might impact training.\n",
    "\n",
    "3. **Implement a Keras Sequence Class**:\n",
    "    - Write a custom `keras.utils.Sequence` class capable of reading the numpy files.\n",
    "    - The class should output:\n",
    "      - The input image.\n",
    "      - The label mask of the lung.\n",
    "    - Bonus: Extend the implementation to support multi-class segmentation for those interested in advanced tasks.\n",
    "\n",
    "4. **Document Your Workflow**:\n",
    "    - For each step, write a brief explanation of what you did and why.\n",
    "    - Include any challenges you faced and how you resolved them.\n",
    "\n",
    "These exercises will help you understand the data preparation process and set the foundation for training a robust neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMO+6euzZoJHVynxTIn5j9z",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
